{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GISCUP 2023 Solution\n",
    "\n",
    "Authors: Adam Booth, Richard Burke\n",
    "\n",
    "Contact: a.booth2@newcastle.ac.uk, r.burke1@newcastle.ac.uk\n",
    "\n",
    "This notebook provides the solution we developed as a team for the GISCUP 2023 competition.\n",
    "\n",
    "The notebook extracts train/test regions from the provided Sentinel-2 imagergy and proceeds to generate lake delineated bitmasks on the test regions only.\n",
    "\n",
    "Once the bitmasks are generated, a post-processing step is taken, to filter out lakes which do not meet the requirements and generates a shapefile containing the detected lakes.\n",
    "\n",
    "The model imported in this notebook (RGB_256_filters_4) is a Tensorflow/Keras model, trained on the provided training regions and can be found in the Model_Training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.features import rasterize\n",
    "import math\n",
    "from rasterio.windows import Window\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "from functools import reduce\n",
    "from osgeo import gdal, ogr, osr\n",
    "from shapely.geometry import LineString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders to work in\n",
    "ROOT_DIR = Path(\"../data/\")\n",
    "DATA_PATH = Path(\"../data/2023_SIGSPATIAL_Cup_data_files/\")\n",
    "TEMP_DIR = ROOT_DIR/\"temp\"\n",
    "if not TEMP_DIR.exists():\n",
    "    TEMP_DIR.mkdir()\n",
    "\n",
    "TRAIN_REGIONS_EXT_DIR = TEMP_DIR/\"regions/train\"\n",
    "TEST_REGIONS_EXT_DIR = TEMP_DIR/\"regions/test\"\n",
    "\n",
    "if not TRAIN_REGIONS_EXT_DIR.exists():\n",
    "    TRAIN_REGIONS_EXT_DIR.mkdir(parents=True)\n",
    "\n",
    "if not TEST_REGIONS_EXT_DIR.exists():\n",
    "    TEST_REGIONS_EXT_DIR.mkdir(parents=True)\n",
    "\n",
    "TEST_REGIONS_BMASK_DIR = ROOT_DIR/\"lakes_bitmask/test_regions\"\n",
    "if not TEST_REGIONS_BMASK_DIR.exists():\n",
    "    TEST_REGIONS_BMASK_DIR.mkdir(parents=True)\n",
    "\n",
    "POST_PROCESSING_DIR = Path(\"../data/post_processing\")\n",
    "if not POST_PROCESSING_DIR.exists():\n",
    "    POST_PROCESSING_DIR.mkdir(parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load regions and sentinal data\n",
    "regions = gpd.read_file(DATA_PATH/\"lakes_regions.gpkg\")\n",
    "sentinal_files = list(Path(DATA_PATH).glob(\"*.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_regions = [\"2019-06-03_2\", \"2019-06-03_4\", \"2019-06-03_6\",\n",
    "                 \"2019-06-19_1\", \"2019-06-19_3\", \"2019-06-19_5\",\n",
    "                 \"2019-07-31_2\", \"2019-07-31_4\", \"2019-07-31_6\",\n",
    "                 \"2019-08-25_1\", \"2019-08-25_3\", \"2019-08-25_5\"]\n",
    "\n",
    "test_regions = [\"2019-06-03_1\", \"2019-06-03_3\", \"2019-06-03_5\",\n",
    "                \"2019-06-19_2\", \"2019-06-19_4\", \"2019-06-19_6\",\n",
    "                \"2019-07-31_1\", \"2019-07-31_3\", \"2019-07-31_5\",\n",
    "                 \"2019-08-25_2\", \"2019-08-25_4\", \"2019-08-25_6\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract regions\n",
    "for _, region in regions.iterrows():\n",
    "    for sentinal_file in sentinal_files:\n",
    "        region_num = region[\"region_num\"]\n",
    "        date = str(sentinal_file)[-17: -7]\n",
    "        \n",
    "        with rasterio.open(sentinal_file) as sentinal_image:\n",
    "            region_raw, affine = mask(sentinal_image, shapes=[region.geometry], crop=True)\n",
    "            \n",
    "        file_name = f\"{date}_{region_num}\"\n",
    "\n",
    "        out_dir = TEST_REGIONS_EXT_DIR if file_name in test_regions else TRAIN_REGIONS_EXT_DIR\n",
    "\n",
    "        with rasterio.open(out_dir/f\"{file_name}.tif\", \n",
    "                           \"w\", driver=sentinal_image.driver, crs=sentinal_image.crs, \n",
    "                           transform=affine, height=region_raw.shape[1], width=region_raw.shape[2], \n",
    "                           count=3, dtype=region_raw.dtype) as out:\n",
    "            out.write(region_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lake Delineation using DUCK-Net Model (see 02_model_training.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "model = tf.keras.models.load_model(\"../model/RGB_256_filters_4/\", compile=False)\n",
    "\n",
    "# Generate bitmasks on test regions, using pre-trained model\n",
    "for region_file in TEST_REGIONS_EXT_DIR.glob(\"*.tif\"):\n",
    "\n",
    "    with rasterio.open(region_file) as region_sat:\n",
    "        date, region = (region_file.stem[:10], region_file.stem[11:])\n",
    "        WINDOW_SIZE = 256\n",
    "\n",
    "        num_cols = math.ceil(region_sat.width / WINDOW_SIZE)\n",
    "        num_rows = math.ceil(region_sat.height / WINDOW_SIZE)\n",
    "        window_indices = [(col, row) for col in range(num_cols) for row in range(num_rows)]\n",
    "\n",
    "        out_bitmask = np.zeros((region_sat.height, region_sat.width, 1), dtype=np.uint8)\n",
    "\n",
    "        for col_indx, row_indx in window_indices:\n",
    "            # Read window\n",
    "            window = Window(col_indx * WINDOW_SIZE, row_indx * WINDOW_SIZE, WINDOW_SIZE, WINDOW_SIZE)\n",
    "            window_read = region_sat.read(window=window)\n",
    "\n",
    "            # Reshape and sacle for inference\n",
    "            window_read = np.moveaxis(window_read, 0, 2)\n",
    "            window_read = np.expand_dims(window_read, axis=0)\n",
    "            window_read = window_read / 255\n",
    "\n",
    "            if window_read.shape[1] != WINDOW_SIZE or window_read.shape[2] != WINDOW_SIZE:\n",
    "                continue\n",
    "\n",
    "            # Predict bitmask for slice\n",
    "            pred = model.predict(window_read, verbose=0)\n",
    "            window_bitmask = pred[0]\n",
    "            window_bitmask = np.round(window_bitmask).astype(np.uint8)\n",
    "\n",
    "            out_bitmask[row_indx*WINDOW_SIZE:row_indx*WINDOW_SIZE+WINDOW_SIZE, col_indx*WINDOW_SIZE:col_indx*WINDOW_SIZE+WINDOW_SIZE] = window_bitmask\n",
    "\n",
    "        with rasterio.open(TEST_REGIONS_BMASK_DIR/f\"{date}_{region}.tif\", \"w\", driver=region_sat.driver, crs=region_sat.crs, transform=region_sat.transform, width=region_sat.shape[1], height=region_sat.shape[0], count=1, dtype=out_bitmask.dtype) as out:\n",
    "            out.write(out_bitmask.squeeze(), indexes=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory\n",
    "\n",
    "for tif_file in TEST_REGIONS_BMASK_DIR.glob(\"*.tif\"):\n",
    "\n",
    "    # Open the Raster File\n",
    "    catchment_raster = gdal.Open(str(tif_file))\n",
    "\n",
    " \n",
    "\n",
    "    # Read the Raster Band\n",
    "\n",
    "    band = catchment_raster.GetRasterBand(1)\n",
    "\n",
    "    band.ReadAsArray()\n",
    "\n",
    " \n",
    "\n",
    "    # Get the Projection\n",
    "\n",
    "    proj = catchment_raster.GetProjection()\n",
    "\n",
    "    shp_proj = osr.SpatialReference()\n",
    "\n",
    "    shp_proj.ImportFromWkt(proj)\n",
    "\n",
    " \n",
    "\n",
    "    # Set Up the Output Shapefile\n",
    "\n",
    "    output_file = POST_PROCESSING_DIR / f\"{tif_file.stem}.shp\"\n",
    "\n",
    "    call_drive = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "\n",
    "    if output_file.exists():\n",
    "        call_drive.DeleteDataSource(str(output_file))\n",
    "\n",
    "    create_shp = call_drive.CreateDataSource(str(output_file))\n",
    "\n",
    "    shp_layer = create_shp.CreateLayer(\"layername\", srs=shp_proj)\n",
    "\n",
    " \n",
    "\n",
    "    # Add a Field to the Shapefile\n",
    "\n",
    "    new_field = ogr.FieldDefn(str('value'), ogr.OFTInteger)\n",
    "\n",
    "    shp_layer.CreateField(new_field)\n",
    "\n",
    " \n",
    "\n",
    "    # Convert Raster to training_vector\n",
    "\n",
    "    gdal.Polygonize(band, None, shp_layer, 0, [], callback=None)\n",
    "\n",
    " \n",
    "\n",
    "    # Cleanup\n",
    "\n",
    "    shp_layer = None\n",
    "\n",
    "    create_shp = None\n",
    "\n",
    "    raster = None\n",
    "\n",
    " \n",
    "\n",
    "print(\"All rasters have been converted to shapefiles.\")\n",
    "\n",
    " \n",
    "\n",
    "#Merge shapefiles together and keep filename in row\n",
    "gdfs = []\n",
    "\n",
    " \n",
    "\n",
    "for shp_file in POST_PROCESSING_DIR.glob(\"*.shp\"):\n",
    "\n",
    "    # Read the shapefile into a GeoDataFrame\n",
    "    gdf = gpd.read_file(shp_file)\n",
    " \n",
    "\n",
    "    # Add a new column with the filename (without extension)\n",
    "    gdf['filename'] = shp_file.stem\n",
    "\n",
    "\n",
    "    # Append the GeoDataFrame to the list\n",
    "\n",
    "    gdfs.append(gdf)\n",
    "\n",
    " \n",
    "\n",
    "# Concatenate all GeoDataFrames into a single GeoDataFrame\n",
    "\n",
    "merged_gdf = gpd.pd.concat(gdfs, ignore_index=True)\n",
    "\n",
    " \n",
    "\n",
    "#Create 'region_num' column\n",
    "\n",
    "merged_gdf['region_num'] = merged_gdf['filename'].str[-1]\n",
    "\n",
    " \n",
    "\n",
    "#Modify 'filename' column\n",
    "\n",
    "merged_gdf['filename'] = 'Greenland26X_22W_Sentinel2_' + merged_gdf['filename'].str[:-2]\n",
    "\n",
    " \n",
    "\n",
    "#Append specified suffixes based on the ending of 'filename'\n",
    "\n",
    "conditions = [\n",
    "\n",
    "    merged_gdf['filename'].str.endswith('2019-06-03'),\n",
    "\n",
    "    merged_gdf['filename'].str.endswith('2019-06-19'),\n",
    "\n",
    "    merged_gdf['filename'].str.endswith('2019-07-31'),\n",
    "\n",
    "    merged_gdf['filename'].str.endswith('2019-08-25')\n",
    "\n",
    "]\n",
    "\n",
    "choices = ['_05.tif', '_20.tif', '_25.tif', '_29.tif']\n",
    "\n",
    " \n",
    "\n",
    "merged_gdf['filename'] += np.select(conditions, choices, default='')\n",
    "\n",
    " \n",
    "\n",
    "#Rename 'filename' column to 'image'\n",
    "\n",
    "merged_gdf = merged_gdf.rename(columns={'filename': 'image'})\n",
    "\n",
    " \n",
    "\n",
    "# Display the modified GeoDataFrame\n",
    "\n",
    "print(merged_gdf)\n",
    "\n",
    " \n",
    "\n",
    "#Tidy up merged_gdf.\n",
    "\n",
    "merged_gdf = merged_gdf[merged_gdf['value'] != 0] #Remove zero value rows\n",
    "\n",
    "merged_gdf = gpd.clip(merged_gdf, regions) #Clip to\n",
    "\n",
    "merged_gdf = merged_gdf.explode() #Explode the MultiPolygons into individual Polygons\n",
    "\n",
    " \n",
    "\n",
    "#Fill holes in merged_gdf\n",
    "\n",
    "sizelim = 100000000000 #Fill holes less than x m2\n",
    "\n",
    "def fillit(row):\n",
    "\n",
    "    newgeom=None\n",
    "\n",
    "    rings = [i for i in row[\"geometry\"].interiors] #List all interior rings\n",
    "\n",
    "    if len(rings)>0: #If there are any rings\n",
    "\n",
    "        to_fill = [Polygon(ring) for ring in rings if Polygon(ring).area<sizelim] #List the ones to fill\n",
    "\n",
    "        if len(to_fill)>0: #If there are any to fill\n",
    "\n",
    "            newgeom = reduce(lambda geom1, geom2: geom1.union(geom2),[row[\"geometry\"]]+to_fill) #Union the original geometry with all holes\n",
    "\n",
    "    if newgeom:\n",
    "\n",
    "        return newgeom\n",
    "\n",
    "    else:\n",
    "\n",
    "        return row[\"geometry\"]\n",
    "\n",
    " \n",
    "\n",
    "#Remove long and narrow polygons\n",
    "\n",
    "def is_lake(polygon):\n",
    "\n",
    "    # Compute the minimum rotated rectangle\n",
    "\n",
    "    min_rotated_rect = polygon.minimum_rotated_rectangle\n",
    "\n",
    " \n",
    "\n",
    "    # Calculate width and height\n",
    "\n",
    "    coords = list(min_rotated_rect.exterior.coords)\n",
    "\n",
    "    width = ((coords[0][0] - coords[1][0]) ** 2 + (coords[0][1] - coords[1][1]) ** 2) ** 0.5\n",
    "\n",
    "    height = ((coords[1][0] - coords[2][0]) ** 2 + (coords[1][1] - coords[2][1]) ** 2) ** 0.5\n",
    "\n",
    " \n",
    "\n",
    "    # Calculate the ratio\n",
    "\n",
    "    ratio = min(width, height) / max(width, height)\n",
    "\n",
    " \n",
    "\n",
    "    # Return whether the polygon is a lake\n",
    "\n",
    "    return ratio >= 0.1\n",
    "\n",
    " \n",
    "\n",
    "# Apply the functions to each row and filter the GeoDataFrame\n",
    "\n",
    "merged_gdf[\"geometry\"] = merged_gdf.apply(fillit, axis=1)\n",
    "\n",
    "merged_gdf['is_lake'] = merged_gdf['geometry'].apply(is_lake)\n",
    "\n",
    "merged_gdf = merged_gdf[merged_gdf['is_lake']]\n",
    "\n",
    " \n",
    "\n",
    "#Calculate area for the filled polygons\n",
    "\n",
    "merged_gdf['area'] = merged_gdf.geometry.area\n",
    "\n",
    "merged_gdf['area'] = merged_gdf['area'].astype(int)\n",
    "\n",
    " \n",
    "\n",
    "#Now remove any rows below 100000m2\n",
    "\n",
    "merged_gdf = merged_gdf[(merged_gdf['area'] > 100000)]\n",
    "\n",
    " \n",
    "\n",
    "merged_gdf = merged_gdf.drop(columns=['area', 'is_lake', 'value'])\n",
    "\n",
    " \n",
    "\n",
    "#Export to geopackage\n",
    "\n",
    "merged_gdf.to_file(\"../data/lake_polygons_test.gpkg\", layer='lakes', driver=\"GPKG\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "giscup3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
